---
title: "Chicago Budget Analysis"
author: "Michael Molloy"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    highlight: tango
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
    includes:
          after_body: "./assets/html/footer.html"
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r include=FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(kableExtra)
library(stringdist)
library(reshape2)
library(stringr)
```
#Chicago Emplpyee Data 
The City of Chicago publishes dozens of datasets every year through their data portal (https://data.cityofchicago.org/).  Since 2011, they have made public the salaries/ hourly rates for all City of Chicago employees.

# Loading Source Data
Source data was exported from the Chicago data portal into CSV format. The file was loaded into R as a dataframe. 
```{r}
# read data
Employee <- read_csv("./input/source/employees/Current_Employee_Names__Salaries__and_Position_Titles.csv")
```

# Data Wrangling
Before any analysis can be performed on the dataset we need to tidy up the data. 

```{r}
str(Employee)
```

## Mismatched Department Names
Over time, the city of Chicago has changed the exact text they use to describe departments.  While the numerical department code has stayed the same, it would be helpful if the department name text was consistent in our dataset. 

The graphic below shows the number of unique department names. 

```{r}
# are department names consistent?
Employee %>% 
  distinct(Department) 
```

To make it so the department names are consistent across years, we create a CSV file to map the original text and the text we want to change it to. Then a for loop is used to go through every record in the dataset to find and replace based on our mapping CSV file.

```{r}
# make department names consistent
  ## load the mapping table
  DepartmentNameMap <- read.csv("./input/mapping/Department Mapping Table.csv",stringsAsFactors = F)

  ## use the mapping table to replace the production data
    for (row in 1:NROW(DepartmentNameMap)) {
      from <- DepartmentNameMap[row,1]
      to <- DepartmentNameMap[row,2]
      Employee$Department <- str_replace(Employee$Department,from,to)
    }
```

## Mismatched Job Titles
For every position there is a human readable job title.  Using clustering, we can identify potential duplicates or issues with the data that will make grouping more difficult later on.

```{r}
# select the field in the dataset with which you want to perform clustering
Field <- head(Employee$`Job Titles`, 1000)

# create the stringdistmatrix using the Jaroâ€“Winkler distance algorithm 
UniqueField <- unique(as.character(Field))

DistanceField <- stringdistmatrix(UniqueField,UniqueField,method = "jw") # you can experiment with changing the method to get better results

rownames(DistanceField) <- UniqueField

# loop the clustering algorithm until it reaches the desired average number of records. the thought being that if we have a dataset with records we expect to be mostly unique than the average records in a cluster will be low.  This loop will determine the right number of clusters for your dataset.

  # set starting number of clusters
  i <- 1
  
  # set a starting avg_cluster (should be greater than x)
  avg_cluster <- 10
  
  # this one is important. how many records do you expect to be similar?
  x <- 2
  
  # define variable for use in clustering algorithm
  hc <- hclust(as.dist(DistanceField))

  # begin the loop
  while (avg_cluster > x) {
     dfClust <- data.frame(UniqueField, cutree(hc, k=i))
     names(dfClust) <- c('UniqueField','cluster')
     avg_cluster <- mean(table(dfClust$cluster))
     i = i+1
     }

# compile the cluster data into a data.frame
t <- table(dfClust$cluster)
t <- cbind(t,t / length(dfClust$cluster))
t <- t[order(t[,2], decreasing=TRUE),]
p <- data.frame(factorName=rownames(t), binCount=t[,1], percentFound=t[,2])


dfClust <- merge(x=dfClust, y=p, by.x = 'cluster', by.y='factorName', all.x=T)

dfClust <- dfClust[rev(order(dfClust$binCount,dfClust$cluster)),] # sort by the size of the cluster bin then the cluster ID

names(dfClust) <-  c('cluster','UniqueField','binCount')

# print cluster results
kable(dfClust[order(dfClust[1:200,3], dfClust[1:200,2], decreasing = T),1:2]) %>%
  kable_styling(bootstrap_options = c("striped","hover", "condensed","responsive"),fixed_thead = T) %>%
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "5m")
  
```

Then, using a for loop, we can replace the Job Title for every record that matches the corresponding Title Code in our mapping file.

```{r}
 # make job titles consistent
  ## load the mapping table
 JobTitleMap <- read_csv("./input/mapping/Job Title Mapping Table.csv")


  ## use the mapping table to replace the production data
    for (row in 1:NROW(JobTitleMap)) {
      TitleDescription <- JobTitleMap[row,1]
      TitleCode <- JobTitleMap[row,2]
 budget[budget$`TITLE CODE` == as.character(TitleCode),"TITLE DESCRIPTION"] <- TitleDescription
    }
```


## Saving the Data
Once we've finished with our data wrangling we can save our tidy data to a single RDS file and begin our analysis.

```{r}
# write the output file
write_rds(budget,"./input/source/budget/Chicago Budget.Rds")
```